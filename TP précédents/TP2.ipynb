{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "TP2.ipynb",
   "provenance": [],
   "collapsed_sections": [
    "HIqZkC3iKtjz",
    "qfm4sXZHLAdi",
    "ORxU4c7KKepG"
   ]
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HIqZkC3iKtjz"
   },
   "source": [
    "# **TP2 - Construction, apprentissage et évaluation de premiers classifieurs supervisés**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qfm4sXZHLAdi"
   },
   "source": [
    "# Context & Objectives\n",
    "\n",
    "This TP aims to make you more familiar with the process of building from scratch a machine learning model. In this TP we will see the famous classification model [Logistic Regression](https://en.wikipedia.org/wiki/Logistic_regression#:~:text=Logistic%20regression%20is%20a%20statistical,a%20form%20of%20binary%20regression), that will be used in a binary classification task with synthetic data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QMId21q3OP9m"
   },
   "source": [
    "# 1. Dataset preparation: generating synthetic data distributed normally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQQUvlIY8zmK"
   },
   "source": [
    "The normal distribution, also known as the Gaussian distribution, is so called because it is based on the Gaussian function. This distribution is defined by two parameters: the mean $\\mu$, i.e. the expected value of the distribution, and the standard deviation $\\sigma^2$, i.e. the expected deviation from the mean. The square of the standard deviation is typically referred to as the variance. We denote this distribution as: $\\mathcal{N}(\\mathbf{\\mu}, \\sigma^2)$.\n",
    "\n",
    "Given this mean and variance, its Probability Densitiy Function (PDF) for a value $x$ is:\n",
    "\n",
    "\\begin{equation}\n",
    "p(x \\mid \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp{ \\left( -\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)} \\tag{eq. 1.1}\n",
    "\\end{equation}\n",
    "\n",
    "We call this distribution the univariate normal because it consists of only one random normal variable. Here is the python code that implements this equation:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GcG-l4fr53yt",
    "ExecuteTime": {
     "end_time": "2024-02-23T08:29:59.409694400Z",
     "start_time": "2024-02-23T08:29:59.381301300Z"
    }
   },
   "source": [
    "def univariate_normal(x, mean, variance):\n",
    "    \"\"\"pdf of the univariate normal distribution.\"\"\"\n",
    "    return ((1. / np.sqrt(2 * np.pi * variance)) * \n",
    "            np.exp(-(x - mean)**2 / (2 * variance)))"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i54f2WsXJl6C"
   },
   "source": [
    "A multivariate normal distribution is a generalization of the one-dimensional (univariate) normal distribution to $d$ dimensions. This distribution has a joint PDF given by\n",
    "\n",
    "\\begin{equation}\n",
    "p(\\mathbf{x} \\mid \\mathbf{\\mu}, \\Sigma) = \\frac{1}{\\sqrt{(2\\pi)^d \\lvert\\Sigma\\rvert}} \\exp{ \\left( -\\frac{1}{2}(\\mathbf{x} - \\mathbf{\\mu})^T \\Sigma^{-1} (\\mathbf{x} - \\mathbf{\\mu}) \\right)} \\tag{eq. 1.2}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{x} \\in \\mathcal{R}^d$ is a random vector of size $d$, $\\mu$ is the mean vector, $\\Sigma$ is the covariance matrix (of size $d \\times d$, symmetric, positive definite), and $| \\Sigma |$ its determinant. In short, we denote this multivariate normal distribution as: $\\mathcal{N}(\\mathbf{\\mu}, \\Sigma)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHh51hTt3hRC"
   },
   "source": [
    "**Question 1.1 (BONUS, the solution is given..)** : write the function `def my_multivariate_normal(x, mean, covariance)` which computes a multivariate normal distribution based on eq. 1.2. This function should be valid for $\\mathbf{x} \\in \\mathcal{R}^d$ with $d > 1$. `mean` needs to be an array with shape $(1,d)$ and `covariance` an array with shape $(d,d)$.\n",
    "\n",
    "Using the following cell, verify that your function gives a similar result to the scipy [multivariate_normal](https://scipy.github.io/devdocs/generated/scipy.stats.multivariate_normal.html) function for $\\textbf{x}=[0,1]^T$."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wH3pXgwRJTOb",
    "ExecuteTime": {
     "end_time": "2024-02-23T08:29:59.429210700Z",
     "start_time": "2024-02-23T08:29:59.395661800Z"
    }
   },
   "source": [
    "def my_multivariate_normal(x, mean, covariance):\n",
    "    \"\"\"pdf of the multivariate normal distribution.\"\"\"\n",
    "    xm = x - mean\n",
    "    d = covariance.shape[0]\n",
    "    \n",
    "    return (1. / (np.sqrt((2 * np.pi)**d * np.linalg.det(covariance))) * \n",
    "            np.exp( - ((xm).T.dot(np.linalg.inv(covariance))).dot(xm) / 2 ))  "
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uUxD-QbTVsan",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9f19fc61-aa14-4147-f28d-632a4bf15f4e",
    "ExecuteTime": {
     "end_time": "2024-02-23T08:30:05.739976500Z",
     "start_time": "2024-02-23T08:29:59.412694400Z"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal as scipy_multivariate_normal\n",
    "import numpy as np\n",
    "\n",
    "print(my_multivariate_normal( np.array([[0.], [1.]]) ,np.array([[20.], [10.]]) , np.array([[10., 0.],[0., 10.]])  ) )\n",
    "print(scipy_multivariate_normal([20, 10], [[10,0], [0, 10]]).pdf([0,1]))"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mscipy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mstats\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m multivariate_normal \u001B[38;5;28;01mas\u001B[39;00m scipy_multivariate_normal\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(my_multivariate_normal( np\u001B[38;5;241m.\u001B[39marray([[\u001B[38;5;241m0.\u001B[39m], [\u001B[38;5;241m1.\u001B[39m]]) ,np\u001B[38;5;241m.\u001B[39marray([[\u001B[38;5;241m20.\u001B[39m], [\u001B[38;5;241m10.\u001B[39m]]) , np\u001B[38;5;241m.\u001B[39marray([[\u001B[38;5;241m10.\u001B[39m, \u001B[38;5;241m0.\u001B[39m],[\u001B[38;5;241m0.\u001B[39m, \u001B[38;5;241m10.\u001B[39m]])  ) )\n",
      "File \u001B[1;32mC:\\Workspace\\WPy64-31090\\python-3.10.9.amd64\\lib\\site-packages\\scipy\\stats\\__init__.py:467\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;124;03m.. _statsrefmanual:\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    462\u001B[0m \n\u001B[0;32m    463\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    465\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_warnings_errors\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (ConstantInputWarning, NearConstantInputWarning,\n\u001B[0;32m    466\u001B[0m                                DegenerateDataWarning, FitError)\n\u001B[1;32m--> 467\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_stats_py\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[0;32m    468\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_variation\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m variation\n\u001B[0;32m    469\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistributions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n",
      "File \u001B[1;32mC:\\Workspace\\WPy64-31090\\python-3.10.9.amd64\\lib\\site-packages\\scipy\\stats\\_stats_py.py:39\u001B[0m\n\u001B[0;32m     36\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlib\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m NumpyVersion\n\u001B[0;32m     37\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtesting\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m suppress_warnings\n\u001B[1;32m---> 39\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mscipy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mspatial\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistance\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m cdist\n\u001B[0;32m     40\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mscipy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mndimage\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _measurements\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mscipy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_lib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_util\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (check_random_state, MapWrapper,\n\u001B[0;32m     42\u001B[0m                               rng_integers, _rename_parameter)\n",
      "File \u001B[1;32mC:\\Workspace\\WPy64-31090\\python-3.10.9.amd64\\lib\\site-packages\\scipy\\spatial\\__init__.py:105\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;124;03m=============================================================\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;124;03mSpatial algorithms and data structures (:mod:`scipy.spatial`)\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    102\u001B[0m \u001B[38;5;124;03m   QhullError\u001B[39;00m\n\u001B[0;32m    103\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m--> 105\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_kdtree\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[0;32m    106\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_ckdtree\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[0;32m    107\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_qhull\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n",
      "File \u001B[1;32mC:\\Workspace\\WPy64-31090\\python-3.10.9.amd64\\lib\\site-packages\\scipy\\spatial\\_kdtree.py:5\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mwarnings\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_ckdtree\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m cKDTree, cKDTreeNode\n\u001B[0;32m      7\u001B[0m __all__ \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mminkowski_distance_p\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mminkowski_distance\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m      8\u001B[0m            \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdistance_matrix\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m      9\u001B[0m            \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mRectangle\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mKDTree\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mminkowski_distance_p\u001B[39m(x, y, p\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m):\n",
      "File \u001B[1;32m_ckdtree.pyx:10\u001B[0m, in \u001B[0;36minit scipy.spatial._ckdtree\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mC:\\Workspace\\WPy64-31090\\python-3.10.9.amd64\\lib\\site-packages\\scipy\\sparse\\__init__.py:283\u001B[0m\n\u001B[0;32m    278\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_arrays\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m    279\u001B[0m     csr_array, csc_array, lil_array, dok_array, coo_array, dia_array, bsr_array\n\u001B[0;32m    280\u001B[0m )\n\u001B[0;32m    282\u001B[0m \u001B[38;5;66;03m# For backward compatibility with v0.19.\u001B[39;00m\n\u001B[1;32m--> 283\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m csgraph\n\u001B[0;32m    285\u001B[0m \u001B[38;5;66;03m# Deprecated namespaces, to be removed in v2.0.0\u001B[39;00m\n\u001B[0;32m    286\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m    287\u001B[0m     base, bsr, compressed, construct, coo, csc, csr, data, dia, dok, extract,\n\u001B[0;32m    288\u001B[0m     lil, sparsetools, sputils\n\u001B[0;32m    289\u001B[0m )\n",
      "File \u001B[1;32mC:\\Workspace\\WPy64-31090\\python-3.10.9.amd64\\lib\\site-packages\\scipy\\sparse\\csgraph\\__init__.py:182\u001B[0m\n\u001B[0;32m    154\u001B[0m __docformat__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrestructuredtext en\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    156\u001B[0m __all__ \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mconnected_components\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m    157\u001B[0m            \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlaplacian\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m    158\u001B[0m            \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mshortest_path\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    179\u001B[0m            \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcsgraph_to_masked\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m    180\u001B[0m            \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNegativeCycleError\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m--> 182\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_laplacian\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m laplacian\n\u001B[0;32m    183\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_shortest_path\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m    184\u001B[0m     shortest_path, floyd_warshall, dijkstra, bellman_ford, johnson,\n\u001B[0;32m    185\u001B[0m     NegativeCycleError\n\u001B[0;32m    186\u001B[0m )\n\u001B[0;32m    187\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_traversal\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m    188\u001B[0m     breadth_first_order, depth_first_order, breadth_first_tree,\n\u001B[0;32m    189\u001B[0m     depth_first_tree, connected_components\n\u001B[0;32m    190\u001B[0m )\n",
      "File \u001B[1;32mC:\\Workspace\\WPy64-31090\\python-3.10.9.amd64\\lib\\site-packages\\scipy\\sparse\\csgraph\\_laplacian.py:7\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mscipy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msparse\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m isspmatrix\n\u001B[1;32m----> 7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mscipy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msparse\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlinalg\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LinearOperator\n\u001B[0;32m     10\u001B[0m \u001B[38;5;66;03m###############################################################################\u001B[39;00m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m# Graph laplacian\u001B[39;00m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlaplacian\u001B[39m(\n\u001B[0;32m     13\u001B[0m     csgraph,\n\u001B[0;32m     14\u001B[0m     normed\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     21\u001B[0m     symmetrized\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m     22\u001B[0m ):\n",
      "File \u001B[1;32mC:\\Workspace\\WPy64-31090\\python-3.10.9.amd64\\lib\\site-packages\\scipy\\sparse\\linalg\\__init__.py:120\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;124;03mSparse linear algebra (:mod:`scipy.sparse.linalg`)\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;124;03m==================================================\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    117\u001B[0m \n\u001B[0;32m    118\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m--> 120\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_isolve\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[0;32m    121\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_dsolve\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[0;32m    122\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_interface\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n",
      "File \u001B[1;32mC:\\Workspace\\WPy64-31090\\python-3.10.9.amd64\\lib\\site-packages\\scipy\\sparse\\linalg\\_isolve\\__init__.py:4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIterative Solvers for Sparse Linear Systems\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m#from info import __doc__\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01miterative\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mminres\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m minres\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlgmres\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m lgmres\n",
      "File \u001B[1;32mC:\\Workspace\\WPy64-31090\\python-3.10.9.amd64\\lib\\site-packages\\scipy\\sparse\\linalg\\_isolve\\iterative.py:9\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtextwrap\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dedent\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[1;32m----> 9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _iterative\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mscipy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msparse\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlinalg\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_interface\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LinearOperator\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m make_system\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRfNhTDGlCW5"
   },
   "source": [
    "Below, we compute and plot two bivariate distributions $\\mathcal{N_1}(\\mathbf{\\mu_1}, \\Sigma_1)$ and $\\mathcal{N_2}(\\mathbf{\\mu_2}, \\Sigma_2)$, where $\\mu_1 =\n",
    "\\begin{bmatrix}\n",
    "20 \\\\\n",
    "10\n",
    "\\end{bmatrix}\n",
    "$\n",
    "and $\\mu_2 =\n",
    "\\begin{bmatrix}\n",
    "20 \\\\\n",
    "10\n",
    "\\end{bmatrix}\n",
    "$\n",
    ", and the $\\Sigma_1$ and $\\Sigma_2$ will have to be defined so that the two variables $x_1$ and $x_2$ are independent in $\\mathcal{N_1}$, and correlated in $\\mathcal{N_2}$."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "recwKgBujCvE",
    "ExecuteTime": {
     "end_time": "2024-02-23T08:30:05.757471300Z",
     "start_time": "2024-02-23T08:30:05.741971800Z"
    }
   },
   "source": [
    "from matplotlib import cm # Colormaps\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Plot bivariate distribution\n",
    "def generate_surface(mean, covariance, nb_of_x):\n",
    "    # nb_of_x is the grid size\n",
    "    \"\"\"Helper function to generate density surface.\"\"\"\n",
    "    x1s = np.linspace(10, 30, num=nb_of_x)\n",
    "    x2s = np.linspace(0,20, num=nb_of_x)\n",
    "    x1, x2 = np.meshgrid(x1s, x2s) # Generate grid\n",
    "    pdf = np.zeros((nb_of_x, nb_of_x))\n",
    "    # Fill the cost matrix for each combination of weights\n",
    "    for i in range(nb_of_x):\n",
    "        for j in range(nb_of_x):\n",
    "          pdf[i,j] = my_multivariate_normal( np.matrix([[x1[i,j]], [x2[i,j]]]), mean, covariance )\n",
    "    return x1, x2, pdf "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VU7Y35fPJZEE",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "outputId": "37a8c17b-5a27-4818-fc8b-215a82a686a6",
    "ExecuteTime": {
     "start_time": "2024-02-23T08:30:05.744969700Z"
    }
   },
   "source": [
    "nb_of_x = 100 # grid size\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(8,4))\n",
    "    \n",
    "# Plot of uncorrelated Normals\n",
    "x1, x2, p = generate_surface( np.matrix([[20.], [10.]]) , np.matrix([[1., 0.],[0., 1.]])*10 , nb_of_x)\n",
    "# Plot bivariate distribution\n",
    "con = ax1.contourf(x1, x2, p, 100, cmap=cm.YlGnBu)\n",
    "ax1.set_xlabel('$x_1$', fontsize=13)\n",
    "ax1.set_ylabel('$x_2$', fontsize=13)\n",
    "# ax1.axis([-2.5, 2.5, -2.5, 2.5])\n",
    "ax1.set_aspect('equal')\n",
    "ax1.set_title('Independent variables', fontsize=12)\n",
    "\n",
    "# Plot of correlated Normals\n",
    "x1, x2, p = generate_surface( np.matrix([[20.], [10.]]) , np.matrix([[1., 0.8],[0.8, 1.]])*10, nb_of_x)\n",
    "# Plot bivariate distribution\n",
    "con = ax2.contourf(x1, x2, p, 100, cmap=cm.YlGnBu)\n",
    "ax2.set_xlabel('$x_1$', fontsize=13)\n",
    "ax2.set_ylabel('$x_2$', fontsize=13)\n",
    "# ax2.axis([-2.5, 2.5, -1.5, 3.5])\n",
    "ax2.set_aspect('equal')\n",
    "ax2.set_title('Correlated variables', fontsize=12)\n",
    "\n",
    "# Add colorbar and title\n",
    "fig.subplots_adjust(right=0.8)\n",
    "cbar_ax = fig.add_axes([0.85, 0.15, 0.02, 0.7])\n",
    "cbar = fig.colorbar(con, cax=cbar_ax)\n",
    "cbar.ax.set_ylabel('$p(x_1, x_2)$', fontsize=13)\n",
    "plt.suptitle('Bivariate normal distributions', fontsize=13, y=0.95)\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9kPRpNarnKP"
   },
   "source": [
    "**Question 1.3**: create a synthetic dataset for a binary classification task, containing normally distributed data with $n=5000$ samples per class and $d=2$ features per sample. Using the `scatter` function, plot the feature matrix $X$ against its two dimensions. Choose the parameters mean and covariance so that your two distributions are readily separable to the naked eye. \n",
    "\n",
    "*tips 1*: to build the feature matrix $X$ you can use the numpy [multivariate_normal](https://numpy.org/doc/stable/reference/random/generated/numpy.random.multivariate_normal.html) function (yet another python function to generate normal distributions!). \n",
    "\n",
    "*tips 2*: your label vector *y* is a binary variable, so it should contain only 0 and 1 values. To visualize your distributions, do not forget to use the *c* parameters of the function scatter.\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "n = 5000\n",
    "mean1 = [20, 10]\n",
    "cov1 = [[1, 0], [0, 1]]\n",
    "mean2 = [25, 15]\n",
    "cov2 = [[1, 0.8], [0.8, 1]]\n",
    "\n",
    "X1 = np.random.multivariate_normal(mean1, cov1, n)\n",
    "X2 = np.random.multivariate_normal(mean2, cov2, n)\n",
    "\n",
    "X = np.concatenate((X1, X2), axis = 0)\n",
    "y = np.concatenate((np.zeros(n), np.ones(n)))\n",
    "\n",
    "plt.scatter(X1[:, 0], X1[:,1], c='b', label='Class 0')\n",
    "plt.scatter(X2[:,0], X2[:,1], c='r', label='Class 1')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Question 1.3 - Feature matrix X')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-23T08:30:05.750073500Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mvPLhh3wTiWz"
   },
   "source": [
    "**Question 1.4**: create a partition of the dataset that splits it into training / test sets with a ratio of 90 / 10 %, using the scikit [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LjAe5m8PNQnJ"
   },
   "source": [
    "# 2. Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpe5GQDhZRp4"
   },
   "source": [
    "## Model overview\n",
    "\n",
    "In a supervised learning problem, our goal is to learn iteratively a function $h:\\boldsymbol{X} → y$ so that $h$ is a \"good predictor\" of a target $y$ (a.k.a label vector) based on the feature matrix $\\boldsymbol{X}$. $h$ is known as the hypothesis function. For example, the hypothesis function of a [linear regression model](https://en.wikipedia.org/wiki/Linear_regression) is simply \n",
    "\n",
    "$$h(\\boldsymbol{X}) = \\boldsymbol{X} \\boldsymbol{\\theta}$$\n",
    "\n",
    "where $\\boldsymbol{\\theta} = [\\theta_0 , \\theta_1 , \\cdots , \\theta_d ]^T$ are the coefficients of the model. Besides a hypothesis function, any machine learning model sets up a cost function $J(\\boldsymbol{\\theta})$ used to estimate the coefficients $\\boldsymbol{\\theta}$. This estimation most often consists in minimizing $J(\\boldsymbol{\\theta})$ through optimization algorithms such as the descent gradient algorithm.\n",
    "\n",
    "Logistic regression is a machine learning classification algorithm that is used to model the probability of a categorical dependent variable. This type of variables here refers to as a binary, ordinal, nominal or event count variable. In the following, we will deal with a binary classification task, using the dataset prepared in question 1.4.\n",
    "\n",
    "The hypothesis function of a logistic regression is obtained by applying a sigmoid function to the output of the linear regression, that is\n",
    "\n",
    "$$h(\\boldsymbol{X}) = \\sigma{ ( \\boldsymbol{X} \\boldsymbol{\\theta} ) }$$\n",
    "\n",
    "where  $\\sigma$ is the sigmoid function defined as $$ \\sigma{(t)} = \\frac{1}{1 + e^{-t} }$$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6vyQCP_A8rlz"
   },
   "source": [
    "## Model implementation\n",
    "\n",
    "**Question 2.1**: let's first write two functions `def sigmoid(x):` that computes a sigmoid and `def h(x,theta):` that computes the hypothesis function.\n",
    "\n",
    "Plot a sigmoid function for x ranging from -10 to 10. Above which value of this function the predicted class will be 1 ? \n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def h(x, theta):\n",
    "    return sigmoid(x * theta.T)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-23T08:30:05.754460600Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bliI3wwnSb9h"
   },
   "source": [
    "**Question 2.2** : we will now have to define the cost function $J(\\boldsymbol{\\theta})$ for our model. For a logistic regression, it is based on the following system of equations:\n",
    "\n",
    "$$ \n",
    "    J(\\boldsymbol{\\theta})=\\left\\{\n",
    "                \\begin{array}{ll}\n",
    "                  - log(h_{ \\theta }(\\boldsymbol{x})), \\quad if \\quad y=1\\\\\n",
    "                  - log(1-h_{ \\theta }(\\boldsymbol{x})), \\quad if \\quad y=0\n",
    "                \\end{array}\n",
    "              \\right.\n",
    "$$\n",
    "\n",
    "Write a function implementing the cost function $J(\\boldsymbol{\\theta})$ of a logistic regression for $n$ training samples by combining these two functions. \n",
    "\n",
    "*tips 1*: start by plotting these two functions on a same graph (remember $h(x) \\rightarrow [0,1]$)\n",
    "\n",
    "*tips 2*: remember that a cost function should be highly penalized (i.e. getting a very large value) if the model predicts the wrong class\n",
    "\n",
    "*tips 3*: split your problems into the different cases. A first case would be where you have $y=1$ and $h(x)=1$, and a second case where you still have $y=1$ but $h(x)=0$. What would be the cost values in both cases ? What now if you have $y=0$ ?"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def cost_J(theta, X, y):\n",
    "    # X la base d’apprentissage m*(n+1) et non pas l’observation x\n",
    "    #calcul matriciel\n",
    "    first = np.multiply(-y, np.log(h(X, theta)))\n",
    "    second = np.multiply((1 - y), np.log(1 - h(X, theta)))\n",
    "    return np.sum(first - second) / (len(X))\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T08:30:05.761470800Z",
     "start_time": "2024-02-23T08:30:05.759471400Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8UsmJm3wtHqr"
   },
   "source": [
    "**Question 2.3** : the gradient of $J(\\boldsymbol{\\theta})$ is given by $$\\frac{\\partial (J(\\boldsymbol{\\theta}))}{\\partial \\boldsymbol{\\theta}} = \\boldsymbol{x}^T ( h_{ \\theta }(\\boldsymbol{x}) - \\boldsymbol{y})$$\n",
    "\n",
    "Write a function `def gradient(x,y,theta):` that takes as inputs $x$ and $y$, representing respectively your model inputs and outputs from all your training dataset samples, as well as your $\\theta$ parameters to be optimized over this dataset."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def gradient(x, y, theta):\n",
    "    parameters=int(theta.ravel().shape[0])\n",
    "    grad = np.zeros(parameters)\n",
    "    error = h(x, theta) - y\n",
    "    for i in range(parameters):\n",
    "        term = np.multiply(error, x[:,i])\n",
    "        grad[i] = np.sum(term) / len(x)\n",
    "    return grad"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T08:30:05.762473400Z",
     "start_time": "2024-02-23T08:30:05.762473400Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01mxmNofhi3T"
   },
   "source": [
    "**Question BONUS** : try to derive yourself the gradient of $J(\\boldsymbol{\\theta})$, it is quite a long but interesting calcul !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NSII_bsLYSkh"
   },
   "source": [
    "**Question 2.4** : now you can apply your `gradient` function within a classical gradient descent algorithm to estimate the $\\theta$ parameters. Find empirically a minimal number of iterations in your gradient descent to have a \"satisfying\" estimation.\n",
    "\n",
    "*tips* : use your `cost_func` function to compute the values of your cost function at each iteration and plot it. Basically, when the cost reaches a plateau, then your optimization is done.\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-23T08:30:05.868008500Z",
     "start_time": "2024-02-23T08:30:05.765471700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kSrx-4k1-wAn"
   },
   "source": [
    "**Question 2.5** : compare the $\\theta$ values from your logistic regression implementation with the one from the sklearn [LogisticRegressionn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) function.\n",
    "\n",
    "Since sklearn's LogisticRegression automatically does L2 regularization (which we did not implement), set C=1e15 to turn off regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wW0xSrr7vV6o"
   },
   "source": [
    "## Model evaluation\n",
    "\n",
    "**Question 2.6** : compute the accuracy metric for both model implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "igUs3ZK_aT3G"
   },
   "source": [
    "**Question 2.7** : using the sklearn functions [roc_curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html) sklearn function, evaluate the performance of your logistic regression model with respect to its [ROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) curve. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ORxU4c7KKepG"
   },
   "source": [
    "# SOURCES\n",
    "\n",
    "## Synthetic data\n",
    "\n",
    "- https://peterroelants.github.io/posts/multivariate-normal-primer/\n",
    "\n",
    "- https://jrnold.github.io/bayesian_notes/appendix.html\n",
    "\n",
    "\n",
    "## Logistic regression\n",
    "\n",
    "-   https://towardsdatascience.com/building-a-logistic-regression-in-python-301d27367c24\n",
    "\n",
    "-   https://beckernick.github.io/logistic-regression-from-scratch/\n",
    "\n",
    "-   https://colab.research.google.com/github/ArunkumarRamanan/Exercises-Machine-Learning-Crash-Course-Google-Developers/blob/master/logistic_regression.ipynb#scrollTo=JjBZ_q7aD9gh\n"
   ]
  }
 ]
}
