{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9yBro2t4C094"
   },
   "source": [
    "# **TP1 Part 1: le workflow universel du machine learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3FNW770KHG9O",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Context & Objectives\n",
    "\n",
    "This TP aims to familiarize yourself with the \"universal workflow of machine learning\" (this expression is from Chollet et al. (2019) ). This workflow should be used as a step-by-step guideline to reach preliminary results on a new machine learning project. It is composed of the following steps:\n",
    "\n",
    "1.   **Problem definition** : define the machine learning task based on available data, visualize and compute summary statistics on your data to better understand them ;\n",
    "2.   **Dataset preparation** : perform some feature engineering if needed (such as variable selection), format your data in a way that can be fed into a machine-learning model, i.e. as a feature matrix or tensor **X**, and build a vector of labels *y* for supervised tasks, perform feature normalization and selection if needed (done in conjunction with step 3) ;\n",
    "3.   **Evaluation protocol** : split **X** into a train / test partition (we remind you that the goal of this splitting is first to train a classifier by \"fitting\" it on the training subset, and secondly to make this trained classifier predict unseen digits from the test subset), choose performance metrics, define a cross-validation procedure, evaluate a baseline machine learning model (starting with off-the-shelf algorithms adapted to your problem and default model parameters) ;\n",
    "\n",
    "4.   **Model evaluation** : run model training and test w.r.t the evaluation protocol, interpret performance result and re-iterate the worflow from step 2 as necessary.\n",
    "\n",
    "In this TP, we will see how this workflow can be used to achieve first results  towards the development of these two (already existing) industrial products:\n",
    "\n",
    "1.   The [CheckReader™](https://www.a2ia.com/en/a2ia-checkreader-0) system from the company Mitek : it is a global standard for advanced image analysis and intelligent recognition software used to seamlessly, precisely and securely process checks and other payment documentation by banks, financial institutions and other progressive corporations around the world. Here we will focus on the **recognition of handwritten digits from bank checks** ;\n",
    "\n",
    "2.   (BONUS part) The [Faradai Platform](https://faradai.ai/?page=blog&subpage=forecasting-energy-consumption-with-ai) develops SaaS products for energy management. It has various modules to analyze energy usage or energy production ; **Energy Consumption Forecasting** is one of these tools. Forecasting is crucial for energy managers to track the success of energy conservation projects and to calculate the feasibility of energy saving investments. It is also important for energy budget managers to prepare highly accurate budgets for the next quarter or year and to procure cost-effective energy tariffs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Wdj-QLarGP4"
   },
   "source": [
    "# 1. Digit recognition from images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UhHto7DwI_Dc"
   },
   "source": [
    "For this first problem, we will be using the sklearn dataset called `load_digits`, downloaded as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1476,
     "status": "ok",
     "timestamp": 1648198840415,
     "user": {
      "displayName": "Dorian Cazau",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08552727308627398808"
     },
     "user_tz": -60
    },
    "id": "uzv5Ng5DUcXj",
    "outputId": "c1b0aad6-f77e-45af-eaad-fdbcef165d3a"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "print(type(digits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1648198840416,
     "user": {
      "displayName": "Dorian Cazau",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08552727308627398808"
     },
     "user_tz": -60
    },
    "id": "GlgvSjIIeSQD",
    "outputId": "6e648602-6dca-46eb-dce9-b8a111a1c138"
   },
   "outputs": [],
   "source": [
    "digits['images'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1648198840416,
     "user": {
      "displayName": "Dorian Cazau",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08552727308627398808"
     },
     "user_tz": -60
    },
    "id": "XrwRXtG0fijI",
    "outputId": "1b9da943-6773-4c2a-b93e-1fa1cf582751"
   },
   "outputs": [],
   "source": [
    "digits['target'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ngKoMb7P5n7e"
   },
   "source": [
    "and here is what a digit looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 325
    },
    "executionInfo": {
     "elapsed": 720,
     "status": "ok",
     "timestamp": 1648127805005,
     "user": {
      "displayName": "Dorian Cazau",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08552727308627398808"
     },
     "user_tz": -60
    },
    "id": "svI0YG55BrrB",
    "outputId": "0040994b-2b0a-40f9-f0f1-dd1ae13ae154"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure(1, figsize=(10,10))\n",
    "plt.clf()\n",
    "plt.subplots_adjust(left=.01, right=.99, bottom=.01, top=.4)\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(digits.images[10+i], cmap=plt.cm.gray, vmax=16, interpolation='nearest')\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.title('Digit no: %i' %i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "StZCadEKglgz"
   },
   "source": [
    "The object ```digits``` is a bunch object. If this is the first time you hear about it, it is better you have a quick look to its [documentation](https://pypi.org/project/bunch/) before starting... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nLVa9GCsa0u4"
   },
   "source": [
    "## Problem definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0Rm2uoxe3tb"
   },
   "source": [
    "**Question 1.1** : please answer the following questions about the dataset:\n",
    "\n",
    "1. what is the total number of samples in the dataset ?\n",
    "2. what is the number of classes in the dataset ? and the number of samples per class ? Make comments on the balancedness of your dataset. \n",
    "\n",
    "Based on these information, define a machine learning task relevant to your product development.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8DzIVufmtSsU"
   },
   "source": [
    "**Question 1.2** : please answer the following questions about the images:\n",
    "\n",
    "1.   what is the size of an image ? \n",
    "2.   what is the type of a pixel value ? Comment.\n",
    "3.   what is the image type and bit depth ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kpOdMm-YrMY8"
   },
   "source": [
    "## Dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bzU-yyOKhiu"
   },
   "source": [
    "**Question 1.3** : most standard machine learning algorithms impose a 2D feature matrix **X** as input data. Propose a way to build such a matrix for your dataset. In your code it will be a `np.ndarray` called `X`.\n",
    "\n",
    "*tips: you need to flatten the image, i.e. to reshape all images in a `(total_samples, **nb_features**)` matrix, i.e. bis turning each 2-D array of grayscale values from shape (8, 8) into shape (64,).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X5JbRq_CLZEO"
   },
   "source": [
    "## Evaluation protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4OJdr85lLc_y"
   },
   "source": [
    "**Question 1.4** : split `X` into train and test subsets using a 80/20 % train/test ratio. `X` will then be divided into `X_train` and `X_test`, and `y` into the corresponding labels `y_train` and `y_test`. This splitting can be done with the scikit [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function. Take time to make you familiar with the different parameters of this function, especially the `stratify` parameter.\n",
    "\n",
    "What is the size of the whole training/testing datasets and the size of each class? Comment the splitting results.\n",
    "\n",
    "*tips : use the [Counter](https://docs.python.org/fr/3/library/collections.html#collections.Counter) class from the module collections*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YEeSASzJiwto"
   },
   "source": [
    "**Question 1.5** : as evaluation metrics, we will use the n-class confusion matrix and the overall accuracy. This latter is defined as the sum of its diagonal elements divided by the sum of all elements.\n",
    "\n",
    "In a code cell below, write a function `def compute_confusion_matrix(y_test, y_pred), ` where `y_pred` is the predicition by a `classifier` of some target labels `y_test`. We consider that the N classes are labelled with integers from 0 to N-1.\n",
    "\n",
    "To help you, you can compare your function outputs with those from the sklearn function, implemented on the toy example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1648127805988,
     "user": {
      "displayName": "Dorian Cazau",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08552727308627398808"
     },
     "user_tz": -60
    },
    "id": "INsmiIB0kL76",
    "outputId": "dcaccd1a-5c29-478a-f313-a2c215443541"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# print(metrics.accuracy_score(y_test_example,y_pred_example))\n",
    "\n",
    "y_test_example = [0, 1, 2, 0, 1, 2, 0, 1, 2]\n",
    "y_pred_example = [0, 0, 0, 0, 1, 1, 0, 2, 2]\n",
    "\n",
    "result = metrics.confusion_matrix(y_test_example,y_pred_example)\n",
    "res,ov = my_confusion_matrix(y_test_example,y_pred_example)\n",
    "\n",
    "print('Custom confusion matrix: \\n',result)\n",
    "print('Sklearn confusion matrix: \\n',res)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('Custom confusion matrix:',ov)\n",
    "print('Sklearn confusion matrix:',metrics.accuracy_score(y_test_example,y_pred_example)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3x6EK_L9N4m8"
   },
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2TD7vscK_ug"
   },
   "source": [
    "As explained in our introduction, it is good practice to start your evaluation with a simple off-the-shelf model called a baseline. Here we will use a famous classifier called [logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) (weird name for a classifier, but we will clarify this in the next TP)\n",
    "\n",
    "Let's create it and train it using the `X_train` subset. For this first TP, do not worry about it, here is how this is done using `sklearn` tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 756,
     "status": "ok",
     "timestamp": 1648127806733,
     "user": {
      "displayName": "Dorian Cazau",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08552727308627398808"
     },
     "user_tz": -60
    },
    "id": "sP98diBcKthq",
    "outputId": "f0929753-2257-4d6a-b154-6f3afdf96e15"
   },
   "outputs": [],
   "source": [
    "# as we will see in TP2, LogisticRegression is quite tricky to make it optimally converge, this command line ignores warnings related to convergence problem\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create a classifier: a LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "#model = svm.SVC(gamma=0.001)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcmk-NIqiZI9"
   },
   "source": [
    "Once trained, your `model` can be directly used to predict your labels on `X_test` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tUwOJ3uGOBA7"
   },
   "outputs": [],
   "source": [
    "predicted = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "At0rJFBlHrv_"
   },
   "source": [
    "You can verify that this variable has the same shape as `y_test` so they can be directly compared:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1648127806737,
     "user": {
      "displayName": "Dorian Cazau",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08552727308627398808"
     },
     "user_tz": -60
    },
    "id": "xxlnmh7WHzfX",
    "outputId": "b757992e-798d-41e3-83f3-e99b83f968fc"
   },
   "outputs": [],
   "source": [
    "print(y_test.shape)\n",
    "print(predicted.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_w7kXaU3gx9c"
   },
   "source": [
    "Below, you can visualize a few randomly picked images that were either correctly classified (in green) or mis-classified (in red):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 325
    },
    "executionInfo": {
     "elapsed": 944,
     "status": "ok",
     "timestamp": 1648127807668,
     "user": {
      "displayName": "Dorian Cazau",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08552727308627398808"
     },
     "user_tz": -60
    },
    "id": "Vde47lnbcy-w",
    "outputId": "0a28ab20-3dfd-4cfb-9c86-e19f05df0e63"
   },
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(10,10))\n",
    "plt.clf()\n",
    "plt.subplots_adjust(left=.01, right=.99, bottom=.01, top=.4)\n",
    "\n",
    "X_test_CC = X_test[(y_test - predicted == 0)] \n",
    "predicted_CC = predicted[(y_test - predicted == 0)] \n",
    "y_test_CC = y_test[(y_test - predicted == 0)] \n",
    "\n",
    "X_test_MC = X_test[(y_test - predicted != 0)] \n",
    "predicted_MC = predicted[(y_test - predicted != 0)] \n",
    "y_test_MC = y_test[(y_test - predicted != 0)] \n",
    "\n",
    "nber_images=10\n",
    "\n",
    "for i in range(nber_images):\n",
    "\n",
    "    if i <nber_images/2:\n",
    "      ind_test = np.random.randint(0,X_test_CC.shape[0],1)\n",
    "      im = X_test_CC[ind_test,:].reshape(8,8)\n",
    "      pred = predicted_CC[ind_test]\n",
    "      y_t = y_test_CC[ind_test]\n",
    "    else:\n",
    "      ind_test = np.random.randint(0,X_test_MC.shape[0],1)\n",
    "      im = X_test_MC[ind_test,:].reshape(8,8)\n",
    "      pred = predicted_MC[ind_test]\n",
    "      y_t = y_test_MC[ind_test]\n",
    "\n",
    "    ax=plt.subplot(2, nber_images/2, i + 1)\n",
    "    plt.imshow( im , cmap=plt.cm.gray, vmax=16, interpolation='nearest')\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    if y_t==pred:\n",
    "      ax.set_title('true: {} -> pred: {}'.format(y_t,pred), color='g')\n",
    "    else:\n",
    "      ax.set_title('true: {} -> pred: {}'.format(y_t,pred), color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YgWDpJ_IwWHC"
   },
   "source": [
    "**Question 1.6** : let's now compute quantitative results on the test subset `y_test` using the n-class confusion matrix and overall accuracy implemented above. Evaluate for each class the rate of correct classification and comment the obtained results.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2PQXt5iSK3G9"
   },
   "source": [
    "**Question 1.7** : perform the same operations as in Question 1.6 but now using the training subset `y_train`. Comment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xa_1Nabqxvj4"
   },
   "source": [
    "**Question 1.8** : within a `for` loop, repeat 5 times the splitting and evaluation processes (respectively from questions 1.4 and 1.6), and compute the median and standard deviation of the 5 resulting overall accuracies.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CXQMDMLQJas"
   },
   "source": [
    "**Question 1.9** : the operation performed in question 1.8 is called a 5-fold cross validation, implemented in `sklearn` with the methods [KFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) and `cross_val_score`. Reproduce your results from question 1.8 using this tool. Compare and comment the results with those of question 1.8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OhlXryBzVDZ3"
   },
   "source": [
    "**Question 1.10** : as a supplementary evaluation, apply basic transformations (e.g. negative, rotation, contrast reduction ..) to input images and see how it impacts predictive performance of the trained model. Comment some results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5PqoAyVyk9I"
   },
   "source": [
    "# SOURCES:\n",
    "\n",
    "*   Chollet, F. (2018) \"[Deep learning with python](https://www.manning.com/books/deep-learning-with-python)\", Manning Publications **(available at the library of ENSTA Bretagne)**\n",
    "\n",
    "*   https://towardsdatascience.com/time-series-modeling-using-scikit-pandas-and-numpy-682e3b8db8d1\n",
    "\n",
    "*   https://petebankhead.gitbooks.io/imagej-intro/content/chapters/bit_depths/bit_depths.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **TP1 Part 2: Energy consumption prediction from timeseries**\n",
    "\n",
    "**Question 2.1** : for this project, we will be using the [opsd_germany_daily](https://raw.githubusercontent.com/jenfly/opsd/master/opsd_germany_daily.csv) dataset. You can download it using its url and the [pandas.read_csv()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) method from the `pandas` library.\n",
    "As you should see your dataset has been put into a pandas `DataFrame`. The library `pandas` provides a set of open source data analysis and manipulation tool built on top of the Python programming language. `pandas` tools are widely used in data sciences, especially with time series data. So before going further, you will have to get more familiar with `pandas`, take 10 mins to go through the official [tutorial](https://pandas.pydata.org/docs/user_guide/10min.html).\n",
    "## Problem definition\n",
    "**Question 2.2** : please answer the following questions about the dataset:\n",
    "\n",
    "*   how many samples contain the dataset ?\n",
    "*   how many variables ? what are their names ?\n",
    "\n",
    "**Question 2.3** : in the process of defining a machine learning problem, computing summary statistics and/or visualizing different aspects of the dataset are often very useful.\n",
    "\n",
    "*   Compute summary statistics to describe the dataset ;\n",
    "\n",
    "*tips: there is a pandas method doing this right away, go read the [tutorial](https://pandas.pydata.org/docs/user_guide/10min.html) again if you do not have it yet*\n",
    "\n",
    "*   Propose a way to visualize relative variations of the consumption, wind and solar variables against time (this can be done in one line!)\n",
    "\n",
    "*tips: after normalizing between 0 and 1 all your pandas variables, plot them against time in a same figure (all this can be done on a single code line in pandas using the function DataFrame.plot() ! )*\n",
    "## Dataset preparation\n",
    "**Question 2.4** : for time series data, it’s conventional to represent the time component in the index of a DataFrame. Doing this, manipulations can be performed with respect to this element.\n",
    "\n",
    "*   What is the current index of your DataFrame ?\n",
    "\n",
    "*   Using the method `pandas.to_datetime()`, parse the string values of `data['Date']` to timestamps ;\n",
    "\n",
    "*   Using the method `pandas.set_index()`, set `data['Date']` as the index of the `DataFrame` ;\n",
    "\n",
    "*   What is the dtype of `data['Date']` now ?\n",
    "**Question 2.5** : as before, we have to define a feature matrix **X** and a label vector *y*.\n",
    "\n",
    "As a first dummy baseline, we will implement the following simplistic model : one that predicts today's consumption value based on two predictive variables: 1) yesterday's consumption value and 2) difference between yesterday and the day before yesterday's consumption value.\n",
    "\n",
    "Using only `DataFrame.loc` and `DataFrame.drop` methods, build a feature matrix `X` with these two variables, named `Yesterday` and `Yesterday_Diff`.\n",
    "\n",
    "*tips: use the [`shift()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.shift.html) and [`diff()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.diff.html) methods*\n",
    "\n",
    "\n",
    "The label vector `y` will be directly defined as the `Consumption` variable.\n",
    "\n",
    "Be carefull to drop all the NaN values within your dataset (`DataFrame.dropna` method).\n",
    "## Evaluation protocol\n",
    "**Question 2.6** : to build your training and test sets, you will be using 10 years of data for training, i.e. 2006-2016, and last year's data for testing, i.e. 2017. Build the variables `X_train`, `y_train`, `X_test` and `y_test` following this protocol.\n",
    "**Question 2.7** : for what concerns performance measures, we will be using the Root Mean Squared Deviation, defined as\n",
    "\n",
    "\\begin{equation}\n",
    " RMSD = \\sqrt{\\frac{\\sum_{i=1}^{N}{\\Big( x_{1,i} -x_{2,i} \\Big)^2}}{N}}\n",
    "\\end{equation}\n",
    "\n",
    "Create a function `def rmsd(y_true, y_pred):` implementing this metric.\n",
    "## Model evaluation\n",
    "**Question 2.8** : your evaluation will benchmark two machine learning models: a [Lasso linear](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) and a [KNn regressor](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html) model.\n",
    "\n",
    "Drawing from the previous evaluation and using sklearn implementations, perform this evaluation.\n",
    "\n",
    "\n",
    "# SOURCES:\n",
    "\n",
    "*   Chollet, F. (2018) \"[Deep learning with python](https://www.manning.com/books/deep-learning-with-python)\", Manning Publications **(available at the library of ENSTA Bretagne)**\n",
    "\n",
    "*   https://towardsdatascience.com/time-series-modeling-using-scikit-pandas-and-numpy-682e3b8db8d1\n",
    "\n",
    "*   https://petebankhead.gitbooks.io/imagej-intro/content/chapters/bit_depths/bit_depths.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-08T22:41:12.396355600Z",
     "start_time": "2024-02-08T22:41:12.395362400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TP1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
